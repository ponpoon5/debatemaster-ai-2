/**
 * ãƒ—ãƒ­ã‚­ã‚·å¯¾å¿œAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ãƒƒãƒ‘ãƒ¼
 * USE_PROXYãƒ•ãƒ©ã‚°ã«å¿œã˜ã¦ã€ç›´æ¥å‘¼ã³å‡ºã—ã¨ãƒ—ãƒ­ã‚­ã‚·çµŒç”±ã‚’è‡ªå‹•åˆ‡æ›¿
 */

import { GoogleGenAI } from '@google/genai';
import { API_KEY, USE_PROXY, PROXY_URL } from '../../core/config/gemini.config';

/**
 * ãƒ—ãƒ­ã‚­ã‚·çµŒç”±ã§APIå‘¼ã³å‡ºã—ã‚’è¡Œã†ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
 */
class ProxyAIClient {
  async generateContent(params: any): Promise<any> {
    const { model, contents, config } = params;

    const response = await fetch(`${PROXY_URL}/api/gemini/generate`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ model, contents, config }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.message || 'Proxy request failed');
    }

    const data = await response.json();
    return {
      text: data.text,
      usageMetadata: data.usageMetadata,
      response: {
        text: () => data.text,
        usageMetadata: data.usageMetadata,
      },
    };
  }

  async *generateContentStream(params: any): AsyncGenerator<any> {
    const { model, contents, config } = params;

    const response = await fetch(`${PROXY_URL}/api/gemini/generate-stream`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ model, contents, config }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(error.message || 'Proxy stream request failed');
    }

    const reader = response.body?.getReader();
    if (!reader) throw new Error('No reader available');

    const decoder = new TextDecoder();

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n');

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') return;

            try {
              const parsed = JSON.parse(data);
              if (parsed.error) {
                throw new Error(parsed.message || 'Stream error');
              }
              yield {
                text: parsed.text || '',
                usageMetadata: parsed.usageMetadata,
              };
            } catch (e) {
              // JSON parse error - skip
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }
  }
}

/**
 * ãƒ¢ãƒ‡ãƒ«ãƒ©ãƒƒãƒ‘ãƒ¼
 */
class ModelsWrapper {
  private directClient: GoogleGenAI | null = null;
  private proxyClient: ProxyAIClient | null = null;

  constructor() {
    if (USE_PROXY) {
      this.proxyClient = new ProxyAIClient();
    } else {
      this.directClient = new GoogleGenAI({ apiKey: API_KEY });
    }
  }

  async generateContent(params: any): Promise<any> {
    if (USE_PROXY && this.proxyClient) {
      return await this.proxyClient.generateContent(params);
    } else if (this.directClient) {
      return await this.directClient.models.generateContent(params);
    }
    throw new Error('No client available');
  }

  async generateContentStream(params: any): AsyncGenerator<any> {
    if (USE_PROXY && this.proxyClient) {
      return this.proxyClient.generateContentStream(params);
    } else if (this.directClient) {
      return this.directClient.models.generateContentStream(params);
    }
    throw new Error('No client available');
  }
}

/**
 * Chatsãƒ©ãƒƒãƒ‘ãƒ¼
 */
class ChatsWrapper {
  private wrapper: AIClientWrapper;

  constructor(wrapper: AIClientWrapper) {
    this.wrapper = wrapper;
  }

  create(options: any) {
    // chats.create()ã¯å†…éƒ¨ã§getGenerativeModel().startChat()ã‚’å‘¼ã¶ã ã‘
    const genModel = this.wrapper.getGenerativeModel({ model: options.model });
    return genModel.startChat({
      history: options.history,
      systemInstruction: options.config?.systemInstruction,
      generationConfig: options.config,
    });
  }
}

/**
 * AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ãƒƒãƒ‘ãƒ¼
 */
class AIClientWrapper {
  models: ModelsWrapper;
  chats: ChatsWrapper;

  constructor() {
    console.log('ğŸ”§ AIClientWrapper constructor called');
    this.models = new ModelsWrapper();
    this.chats = new ChatsWrapper(this);
    console.log('ğŸ”§ chats initialized:', !!this.chats);
  }

  getGenerativeModel(config: { model: string }) {
    // Chatç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿”ã™
    // ç›´æ¥ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯æœ¬ç‰©ã®GoogleGenAIã€ãƒ—ãƒ­ã‚­ã‚·ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ©ãƒƒãƒ‘ãƒ¼
    if (USE_PROXY) {
      return {
        generateContent: async (params: any) => {
          const response = await fetch(`${PROXY_URL}/api/gemini/generate`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              model: config.model,
              contents: params.contents,
              config: params.generationConfig,
            }),
          });

          if (!response.ok) {
            const error = await response.json();
            throw new Error(error.message || 'Proxy request failed');
          }

          const data = await response.json();
          return {
            response: {
              text: () => data.text,
              usageMetadata: data.usageMetadata,
            },
          };
        },
        startChat: (chatParams: any) => {
          // ãƒ—ãƒ­ã‚­ã‚·ãƒ¢ãƒ¼ãƒ‰ç”¨ã®ãƒãƒ£ãƒƒãƒˆãƒ©ãƒƒãƒ‘ãƒ¼
          // æ³¨æ„: sendMessageï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰ã¯ç¾åœ¨ãƒ—ãƒ­ã‚­ã‚·æœªå¯¾å¿œã®ãŸã‚ã€
          // ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§ç›´æ¥Gemini APIã‚’å‘¼ã³å‡ºã™ï¼ˆé–‹ç™ºç’°å¢ƒæ¨å¥¨ï¼‰
          return {
            sendMessage: async (message: any) => {
              const response = await fetch(`${PROXY_URL}/api/gemini/chat`, {
                method: 'POST',
                headers: {
                  'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                  model: config.model,
                  history: chatParams.history || [],
                  message: message.message,
                  config: chatParams.generationConfig,
                }),
              });

              if (!response.ok) {
                const error = await response.json();
                throw new Error(error.message || 'Chat request failed');
              }

              const data = await response.json();
              return {
                text: data.text,
                usageMetadata: data.usageMetadata,
                response: {
                  text: () => data.text,
                  usageMetadata: data.usageMetadata,
                },
              };
            },
            sendMessageStream: async (message: any) => {
              const response = await fetch(`${PROXY_URL}/api/gemini/chat-stream`, {
                method: 'POST',
                headers: {
                  'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                  model: config.model,
                  history: chatParams.history || [],
                  message: message.message,
                  config: chatParams.generationConfig,
                }),
              });

              if (!response.ok) {
                const error = await response.json();
                throw new Error(error.message || 'Chat stream failed');
              }

              const reader = response.body?.getReader();
              if (!reader) throw new Error('No reader available');

              const decoder = new TextDecoder();

              async function* streamGenerator() {
                try {
                  while (true) {
                    const { done, value } = await reader.read();
                    if (done) break;

                    const chunk = decoder.decode(value);
                    const lines = chunk.split('\n');

                    for (const line of lines) {
                      if (line.startsWith('data: ')) {
                        const data = line.slice(6);
                        if (data === '[DONE]') return;

                        try {
                          const parsed = JSON.parse(data);
                          if (parsed.error) {
                            throw new Error(parsed.message || 'Stream error');
                          }
                          yield {
                            text: parsed.text || '',
                            usageMetadata: parsed.usageMetadata,
                          };
                        } catch (e) {
                          // JSON parse error - skip
                        }
                      }
                    }
                  }
                } finally {
                  reader.releaseLock();
                }
              }

              return streamGenerator();
            },
          };
        },
      };
    } else {
      // ç›´æ¥ãƒ¢ãƒ¼ãƒ‰ - GoogleGenAI v1.34.0ã§ã¯getGenerativeModelãŒãªã„ãŸã‚ã€
      // äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
      const directClient = new GoogleGenAI({ apiKey: API_KEY });
      return {
        generateContent: async (params: any) => {
          return await directClient.models.generateContent({
            model: config.model,
            contents: params.contents,
            generationConfig: params.generationConfig,
          });
        },
        startChat: (chatParams: any) => {
          // chatsAPIã‚’ä½¿ã£ã¦ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
          return directClient.chats.create({
            model: config.model,
            history: chatParams.history,
            config: {
              systemInstruction: chatParams.systemInstruction,
              ...chatParams.generationConfig,
            },
          });
        },
      };
    }
  }
}

export const ai = new AIClientWrapper();

// ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
console.log('ğŸ”§ AI Client Mode:', USE_PROXY ? 'PROXY' : 'DIRECT');
console.log('ğŸ”§ API_KEY exists:', !!API_KEY);
console.log('ğŸ”§ PROXY_URL:', PROXY_URL);
