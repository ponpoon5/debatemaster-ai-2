/**
 * ãƒ—ãƒ­ã‚­ã‚·å¯¾å¿œAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ãƒƒãƒ‘ãƒ¼
 * USE_PROXYãƒ•ãƒ©ã‚°ã«å¿œã˜ã¦ã€ç›´æ¥å‘¼ã³å‡ºã—ã¨ãƒ—ãƒ­ã‚­ã‚·çµŒç”±ã‚’è‡ªå‹•åˆ‡æ›¿
 */

import { GoogleGenAI } from '@google/genai';
import { API_KEY, USE_PROXY, PROXY_URL } from '../../core/config/gemini.config';
import type {
  GeminiGenerateContentParams,
  GeminiResponse,
  GeminiStreamChunk,
  ProxyApiResponse,
  ProxyApiError,
} from '../../core/types/gemini-api.types';
import { streamFromProxy } from './utils/streaming-processor';
import { withRetry } from '../api/retry';

/**
 * ãƒ—ãƒ­ã‚­ã‚·çµŒç”±ã§APIå‘¼ã³å‡ºã—ã‚’è¡Œã†ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
 */
class ProxyAIClient {
  async generateContent(params: GeminiGenerateContentParams): Promise<GeminiResponse> {
    const { model, contents, config } = params;

    return withRetry(async () => {
      // PROXY_URLãŒç©ºã®å ´åˆã¯ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆåŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼‰ã‚’ä½¿ç”¨
      const url = PROXY_URL ? `${PROXY_URL}/api/gemini/generate` : '/api/gemini/generate';
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ model, contents, config }),
      });

      if (!response.ok) {
        const error: ProxyApiError = await response.json();
        throw new Error(error.message || 'Proxy request failed');
      }

      const data: ProxyApiResponse = await response.json();
      return {
        text: data.text,
        usageMetadata: data.usageMetadata,
        candidates: data.candidates,
        response: {
          text: () => data.text,
          usageMetadata: data.usageMetadata,
          candidates: data.candidates,
        },
      } as GeminiResponse;
    });
  }

  async *generateContentStream(params: GeminiGenerateContentParams): AsyncGenerator<GeminiStreamChunk> {
    const { model, contents, config } = params;
    // PROXY_URLãŒç©ºã®å ´åˆã¯ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆåŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼‰ã‚’ä½¿ç”¨
    const url = PROXY_URL ? `${PROXY_URL}/api/gemini/generate-stream` : '/api/gemini/generate-stream';
    yield* streamFromProxy(url, { model, contents, config });
  }
}

/**
 * ãƒ¢ãƒ‡ãƒ«ãƒ©ãƒƒãƒ‘ãƒ¼
 */
class ModelsWrapper {
  private directClient: GoogleGenAI | null = null;
  private proxyClient: ProxyAIClient | null = null;

  constructor() {
    if (USE_PROXY) {
      this.proxyClient = new ProxyAIClient();
    } else {
      this.directClient = new GoogleGenAI({ apiKey: API_KEY });
    }
  }

  async generateContent(params: GeminiGenerateContentParams): Promise<GeminiResponse> {
    if (USE_PROXY && this.proxyClient) {
      return await this.proxyClient.generateContent(params);
    } else if (this.directClient) {
      return withRetry(async () => {
        return await this.directClient!.models.generateContent(params) as Promise<GeminiResponse>;
      });
    }
    throw new Error('No client available');
  }

  async generateContentStream(params: GeminiGenerateContentParams): AsyncGenerator<GeminiStreamChunk> {
    if (USE_PROXY && this.proxyClient) {
      return this.proxyClient.generateContentStream(params);
    } else if (this.directClient) {
      return this.directClient.models.generateContentStream(params) as AsyncGenerator<GeminiStreamChunk>;
    }
    throw new Error('No client available');
  }
}

/**
 * Chatsãƒ©ãƒƒãƒ‘ãƒ¼
 */
class ChatsWrapper {
  private wrapper: AIClientWrapper;

  constructor(wrapper: AIClientWrapper) {
    this.wrapper = wrapper;
  }

  create(options: {
    model: string;
    history?: Array<{ role: string; parts: Array<{ text: string }> }>;
    config?: Record<string, unknown>;
  }) {
    // chats.create()ã¯å†…éƒ¨ã§getGenerativeModel().startChat()ã‚’å‘¼ã¶ã ã‘
    const genModel = this.wrapper.getGenerativeModel({ model: options.model });
    return genModel.startChat({
      history: options.history,
      systemInstruction: options.config?.systemInstruction,
      generationConfig: options.config,
    });
  }
}

/**
 * AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ãƒƒãƒ‘ãƒ¼
 */
class AIClientWrapper {
  models: ModelsWrapper;
  chats: ChatsWrapper;

  constructor() {
    console.log('ğŸ”§ AIClientWrapper constructor called');
    this.models = new ModelsWrapper();
    this.chats = new ChatsWrapper(this);
    console.log('ğŸ”§ chats initialized:', !!this.chats);
  }

  getGenerativeModel(config: { model: string }) {
    // Chatç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿”ã™
    // ç›´æ¥ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯æœ¬ç‰©ã®GoogleGenAIã€ãƒ—ãƒ­ã‚­ã‚·ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ©ãƒƒãƒ‘ãƒ¼
    if (USE_PROXY) {
      return {
        generateContent: async (params: { contents: unknown; generationConfig?: unknown; config?: unknown }) => {
          return withRetry(async () => {
            const url = PROXY_URL ? `${PROXY_URL}/api/gemini/generate` : '/api/gemini/generate';
            const response = await fetch(url, {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
              },
              body: JSON.stringify({
                model: config.model,
                contents: params.contents,
                config: params.generationConfig || params.config,
              }),
            });

            if (!response.ok) {
              const error: ProxyApiError = await response.json();
              throw new Error(error.message || 'Proxy request failed');
            }

            const data: ProxyApiResponse = await response.json();
            return {
              text: data.text,
              usageMetadata: data.usageMetadata,
              response: {
                text: () => data.text,
                usageMetadata: data.usageMetadata,
              },
            };
          });
        },
        startChat: (chatParams: {
          history?: Array<{ role: string; parts: Array<{ text: string }> }>;
          systemInstruction?: string;
          generationConfig?: unknown;
        }) => {
          // ãƒ—ãƒ­ã‚­ã‚·ãƒ¢ãƒ¼ãƒ‰ç”¨ã®ãƒãƒ£ãƒƒãƒˆãƒ©ãƒƒãƒ‘ãƒ¼
          // ãƒ­ãƒ¼ã‚«ãƒ«ã§ä¼šè©±å±¥æ­´ã‚’è“„ç©ã—ã€å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§é€ä¿¡ã™ã‚‹
          let accumulatedHistory = chatParams.history || [];

          return {
            sendMessage: async (params: { message: string }) => {
              return withRetry(async () => {
                const url = PROXY_URL ? `${PROXY_URL}/api/gemini/chat` : '/api/gemini/chat';
                const response = await fetch(url, {
                  method: 'POST',
                  headers: {
                    'Content-Type': 'application/json',
                  },
                  body: JSON.stringify({
                    model: config.model,
                    history: accumulatedHistory,
                    message: params.message,
                    config: chatParams.generationConfig,
                  }),
                });

                if (!response.ok) {
                  const error = await response.json();
                  throw new Error(error.message || 'Chat request failed');
                }

                const data = await response.json();

                // å±¥æ­´ã«è¿½åŠ ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼‰
                accumulatedHistory = [
                  ...accumulatedHistory,
                  { role: 'user', parts: [{ text: message.message }] },
                  { role: 'model', parts: [{ text: data.text }] },
                ];

                return {
                  text: data.text,
                  usageMetadata: data.usageMetadata,
                  response: {
                    text: () => data.text,
                    usageMetadata: data.usageMetadata,
                  },
                };
              });
            },
            sendMessageStream: async (params: { message: string }) => {
              const url = PROXY_URL ? `${PROXY_URL}/api/gemini/chat-stream` : '/api/gemini/chat-stream';
              const response = await fetch(url, {
                method: 'POST',
                headers: {
                  'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                  model: config.model,
                  history: accumulatedHistory,
                  message: params.message,
                  config: chatParams.generationConfig,
                }),
              });

              if (!response.ok) {
                const error = await response.json();
                throw new Error(error.message || 'Chat stream failed');
              }

              const reader = response.body?.getReader();
              if (!reader) throw new Error('No reader available');

              const decoder = new TextDecoder();

              async function* streamGenerator() {
                let fullText = '';
                try {
                  while (true) {
                    const { done, value } = await reader.read();
                    if (done) break;

                    const chunk = decoder.decode(value);
                    const lines = chunk.split('\n');

                    for (const line of lines) {
                      if (line.startsWith('data: ')) {
                        const data = line.slice(6);
                        if (data === '[DONE]') {
                          // ã‚¹ãƒˆãƒªãƒ¼ãƒ å®Œäº†æ™‚ã«å±¥æ­´ã«è¿½åŠ 
                          if (fullText) {
                            accumulatedHistory = [
                              ...accumulatedHistory,
                              { role: 'user', parts: [{ text: message.message }] },
                              { role: 'model', parts: [{ text: fullText }] },
                            ];
                          }
                          return;
                        }

                        try {
                          const parsed = JSON.parse(data);
                          if (parsed.error) {
                            throw new Error(parsed.message || 'Stream error');
                          }
                          const chunkText = parsed.text || '';
                          fullText += chunkText;
                          yield {
                            text: chunkText,
                            usageMetadata: parsed.usageMetadata,
                          };
                        } catch (e) {
                          // JSON parse error - skip
                        }
                      }
                    }
                  }

                  // ã‚¹ãƒˆãƒªãƒ¼ãƒ çµ‚äº†æ™‚ã«ã‚‚å±¥æ­´ã«è¿½åŠ ï¼ˆ[DONE]ãŒãªã„å ´åˆï¼‰
                  if (fullText && !accumulatedHistory.some(h => h.role === 'model' && h.parts[0].text === fullText)) {
                    accumulatedHistory = [
                      ...accumulatedHistory,
                      { role: 'user', parts: [{ text: message.message }] },
                      { role: 'model', parts: [{ text: fullText }] },
                    ];
                  }
                } finally {
                  reader.releaseLock();
                }
              }

              return streamGenerator();
            },
          };
        },
      };
    } else {
      // ç›´æ¥ãƒ¢ãƒ¼ãƒ‰ - GoogleGenAI v1.34.0ã§ã¯getGenerativeModelãŒãªã„ãŸã‚ã€
      // äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
      const directClient = new GoogleGenAI({ apiKey: API_KEY });
      return {
        generateContent: async (params: { contents: unknown; generationConfig?: unknown }) => {
          return withRetry(async () => {
            return await directClient.models.generateContent({
              model: config.model,
              contents: params.contents,
              generationConfig: params.generationConfig,
            });
          });
        },
        startChat: (chatParams: {
          history?: Array<{ role: string; parts: Array<{ text: string }> }>;
          systemInstruction?: string;
          generationConfig?: unknown;
        }) => {
          // chatsAPIã‚’ä½¿ã£ã¦ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
          return directClient.chats.create({
            model: config.model,
            history: chatParams.history,
            config: {
              systemInstruction: chatParams.systemInstruction,
              ...chatParams.generationConfig,
            },
          });
        },
      };
    }
  }
}

export const ai = new AIClientWrapper();

// ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
console.log('ğŸ”§ AI Client Mode:', USE_PROXY ? 'PROXY' : 'DIRECT');
console.log('ğŸ”§ API_KEY exists:', !!API_KEY);
console.log('ğŸ”§ PROXY_URL:', PROXY_URL);

// PROXY_URLãŒç©ºã®å ´åˆã¯åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã® /api ã‚’ä½¿ç”¨ï¼ˆVercelç­‰ï¼‰
if (USE_PROXY && !PROXY_URL) {
  console.log('â„¹ï¸ PROXY_URL is empty, using same-origin /api endpoints (Vercel mode)');
}
